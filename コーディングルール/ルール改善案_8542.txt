# コーディングルール改善案

## 発見された問題
NewsSearch_4732.ymlでLLMモデルの最大コンテキスト長（8192トークン）を超過するエラーが発生

## 改善が必要なルール箇所

### 1. 全体コーディングルール.md への追加提案

#### セクション「4.2 ノード定義の完全仕様」の「LLMノード特有の設定」に以下を追加：

```yaml
# トークン数管理（新規追加）
# 重要：モデルのコンテキスト長制限を考慮する必要があります
# GPT-4: 8192トークン（デフォルト）、32768トークン（32k版）
# Claude: 100000トークン
model:
  completion_params:
    max_tokens: 1500  # 出力トークン数
    # 注意：入力トークン + max_tokens < モデルの最大コンテキスト長
```

#### 新規セクション「4.10 トークン数管理とエラー防止」を追加：

```markdown
### 4.10 トークン数管理とエラー防止

#### 4.10.1 LLMノードへの入力サイズ管理
**重要**: LLMノードに大量のテキストを入力する場合、トークン数制限に注意する必要があります。

##### トークン数の目安
- 1トークン ≈ 0.75単語（英語）
- 1トークン ≈ 2-3文字（日本語）
- モデル別制限：
  - GPT-3.5/GPT-4: 8,192トークン（標準）
  - GPT-4-32k: 32,768トークン
  - Claude-2: 100,000トークン

##### トークン数超過を防ぐ設計パターン
1. **検索結果の前処理**
   - 検索結果を直接LLMに渡さない
   - 中間処理ノードで要約・フィルタリング
   
2. **段階的処理**
   ```
   推奨パターン：
   start → query_optimizer → search → preprocessor → analyzer → answer
   
   避けるべきパターン：
   start → search → analyzer → answer
   ```

3. **検索結果の制限**
   - max_resultsを適切に設定（5-10推奨）
   - include_raw_content: 0（生データを含めない）
   - 長文の可能性がある場合は前処理必須

#### 4.10.2 エラー回避のベストプラクティス
1. 外部データ（検索結果、ファイル内容）を扱う場合は必ず中間処理を挟む
2. prompt_template + 入力データ + max_tokens < モデル上限
3. 大量データ処理時は分割処理を検討
```

### 2. コンポーネント記述ルール_WEB検索.txt への追加提案

#### 「注意事項」セクションに以下を追加：

```markdown
6. **トークン数制限への対応**
   - 検索結果は予想以上に長くなる可能性がある
   - 特にtopic: newsやinclude_raw_content: 1の場合は注意
   - 検索結果を直接LLMノードに渡す前に、必ず前処理を検討する
   - 推奨構成：検索 → 前処理/要約 → 最終分析

7. **検索結果の前処理パターン**
   ```yaml
   # パターン1：クエリ最適化を含む完全な処理フロー
   start_node → query_optimizer_node → search_node → preprocessor_node → analyzer_node → answer_node
   
   # パターン2：最小限の前処理
   start_node → search_node → summarizer_node（トークン削減用） → final_analyzer_node → answer_node
   ```
```

### 3. 品質チェックリスト.md への追加提案

#### 「4.2 llmノード」セクションに以下を追加：

```markdown
### 4.2.1 トークン数チェック（新規追加）
- [ ] **外部データ（検索結果、ファイル内容）を直接LLMに渡していない**
- [ ] **prompt_template + 想定される最大入力 + max_tokens < モデル上限**
- [ ] **検索結果を扱う場合、前処理ノードが存在する**
- [ ] **max_tokensが適切に設定されている（通常1000-2000）**
- [ ] **大量データ処理時の分割戦略が実装されている**
```

### 4. クラッシュ事項チェックリスト.md への追加提案

#### 新規セクション「14. トークン数超過によるクラッシュ」を追加：

```markdown
## 14. トークン数超過によるクラッシュ

### 14.1 症状
- "context_length_exceeded"エラー
- "maximum context length is X tokens"メッセージ
- 実行時にLLMノードでエラー発生

### 14.2 原因
- 検索結果や外部データを無加工でLLMに入力
- prompt_templateが長すぎる
- max_tokensの設定が大きすぎる

### 14.3 チェック項目
- [ ] **検索結果を直接LLMノードに渡していない**
- [ ] **ファイル内容を直接LLMノードに渡していない**
- [ ] **前処理ノードで入力サイズを削減している**
- [ ] **モデルのトークン上限を把握している**
- [ ] **max_tokensが控えめに設定されている（1500以下推奨）**

### 14.4 推奨構成
```yaml
# NG：検索結果を直接渡す
start → search → llm（エラーリスク高）

# OK：前処理を挟む
start → search → preprocessor → llm
```
```

## まとめ
今回のエラーは、検索結果のような外部データを無加工でLLMノードに渡したことによるトークン数超過が原因でした。コーディングルールにトークン数管理に関する明確なガイドラインがなかったため、このような実装が生まれたと考えられます。

上記の改善案により、今後同様のエラーを防ぐことができます。特に重要なのは：
1. 外部データは必ず前処理する
2. トークン数制限を常に意識する
3. 推奨される処理フローパターンに従う