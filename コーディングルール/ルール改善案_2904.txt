# Difyコーディングルール改善案

## 改善が必要な理由
NewsSearch_4732.ymlのエラー分析により、モデルのコンテキスト長制限を超えるエラーが発生。
現行のルールには、トークン数管理とモデル選択に関する具体的なガイダンスが不足している。

## 改善案

### 1. 全体コーディングルール.md への追加

#### 追加セクション：「## 20. トークン数管理とモデル選択ガイドライン」

```markdown
## 20. トークン数管理とモデル選択ガイドライン

### 20.1 モデル別コンテキスト長制限
| モデル名 | 最大コンテキスト長 | 推奨用途 |
|---------|-----------------|---------|
| gpt-4 | 8,192トークン | 短い処理、要約タスク |
| gpt-4-turbo | 128,000トークン | 長文処理、大量データ分析 |
| gpt-3.5-turbo | 4,096トークン | 軽量な処理 |
| gpt-3.5-turbo-16k | 16,384トークン | 中規模データ処理 |
| claude-3-opus | 200,000トークン | 超大規模文書処理 |
| claude-3-sonnet | 200,000トークン | 大規模データ処理 |

### 20.2 トークン数計算の目安
- 日本語：1文字 ≈ 0.5〜0.7トークン
- 英語：1単語 ≈ 1〜1.3トークン
- プロンプト + 入力データ + max_tokens ≤ モデルの最大コンテキスト長

### 20.3 WEB検索結果の処理
- Tavily Search結果は予想以上に長くなる可能性がある
- 検索結果を直接LLMに渡す前に、中間処理ノードの追加を検討
- max_resultsの適切な設定（推奨：3〜5）

### 20.4 推奨ワークフロー構成
```
start_node → query_analyzer → web_search → result_preprocessor → llm → answer
```

### 20.5 max_tokensの設定指針
- 安全マージン：(モデル最大長 - 予想入力長) × 0.8
- 最小値：500トークン
- 推奨値：1000〜2000トークン（入力長を考慮）
```

### 2. コンポーネント記述ルール_LLM.txt への追加

#### 「■モデル選択ガイドライン」セクションを追加（completion_paramsの後）

```
■モデル選択ガイドライン

### トークン数によるモデル選択
1. **短い入力（〜2000トークン）**
   - 推奨：gpt-4、gpt-3.5-turbo
   - max_tokens: 1000〜2000

2. **中規模入力（2000〜6000トークン）**
   - 推奨：gpt-4-turbo、gpt-3.5-turbo-16k
   - max_tokens: 1000〜3000

3. **大規模入力（6000トークン以上）**
   - 推奨：gpt-4-turbo、claude-3系
   - max_tokens: 必要に応じて調整

### WEB検索結果を処理する場合の注意
- 検索結果は想定以上に長くなる可能性がある
- gpt-4（8192トークン制限）の使用は避ける
- 推奨：gpt-4-turbo または前処理ノードの追加
```

### 3. コンポーネント記述ルール_WEB検索.txt への追加

#### 「■トークン数管理」セクションを追加（ベストプラクティスの後）

```
■トークン数管理

### 検索結果の長さ制御
1. **max_resultsの設定指針**
   - LLM直接処理：3〜5（推奨）
   - 前処理あり：5〜10
   - 詳細分析：10〜20（要注意）

2. **include_raw_contentの影響**
   - false：要約のみ（推奨）
   - true：全文取得（トークン数大幅増加）

3. **後続ノードへの考慮**
   - 後続のLLMノードのモデル制限を確認
   - 必要に応じて中間処理ノードを追加
   - 例：検索結果要約ノード → 最終分析ノード

### 推奨構成パターン
```yaml
# パターン1：前処理なし（軽量検索）
web_search_node:
  max_results: 3
  include_raw_content: 0
→ llm_node (gpt-4-turbo推奨)

# パターン2：前処理あり（詳細検索）
web_search_node:
  max_results: 10
→ preprocessor_llm_node (結果要約)
→ analyzer_llm_node (最終分析)
```
```

### 4. 品質チェックリスト.md への追加

#### セクション「4.2 llmノード」に以下を追加

```markdown
### 4.2.1 トークン数管理チェック
- [ ] 選択したモデルのコンテキスト長制限を把握している
- [ ] 入力データの予想トークン数を考慮している
- [ ] max_tokens + 入力トークン数 < モデル最大長の80%
- [ ] WEB検索結果を直接処理する場合、大容量モデルを使用している
- [ ] 必要に応じて前処理ノードを追加している
```

### 5. クラッシュ事項チェックリスト.md への追加

#### 新セクション「## 14. トークン数超過によるクラッシュ防止」

```markdown
## 14. トークン数超過によるクラッシュ防止

### 14.1 危険な組み合わせ
- [ ] **gpt-4 + WEB検索結果の直接処理を避けている**
- [ ] **gpt-4 + max_tokens > 2000の組み合わせを避けている**
- [ ] **大量テキスト + 小容量モデルの組み合わせを避けている**

### 14.2 安全性確認
- [ ] **モデルの最大コンテキスト長を確認している**
- [ ] **入力データの最大長を見積もっている**
- [ ] **安全マージン（20%以上）を確保している**

### 14.3 エラー症状
- 症状: "maximum context length is X tokens"エラー
- 原因: 入力 + max_tokens > モデル制限
- 対策: モデル変更または入力削減
```

## 実装優先度
1. 【最優先】コンポーネント記述ルール_LLM.txtへのモデル選択ガイドライン追加
2. 【高】クラッシュ事項チェックリストへのトークン数超過防止項目追加
3. 【中】全体コーディングルールへのトークン数管理セクション追加
4. 【低】その他の改善項目

## 期待される効果
- トークン数超過エラーの予防
- 適切なモデル選択による安定動作
- WEB検索を含むワークフローの信頼性向上
- 開発者への明確なガイダンス提供